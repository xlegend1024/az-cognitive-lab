{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text and Speech Analysis with Cognitive Services\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Text Analyics Cognitive Service\n",
    "\n",
    "Microsoft Cognitive Services includes a Text Analytics service that encapsulates  sophisticated techniques for ascertaining meaning from text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. In Azure Portal navigate to Text Analytics service you created during the lab setup.\n",
    "2. In the blade for your Text Analytics service, click **Keys** and then copy **Key 1** to the clipboard and paste it into the **textKey** variable below. \n",
    "3. Extract URI from **Endpoint** and paste it into the **textAnalyticsURI** variable.\n",
    "3. Run the cell below to assign the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textAnalyticsURI = '<your URI>'\n",
    "textKey = '<your key>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Curl to get a document from GitHub and open it\n",
    "!curl https://raw.githubusercontent.com/MicrosoftLearning/AI-Introduction/master/files/Moon.txt -o Moon.txt\n",
    "doc1 = open(\"Moon.txt\", \"r\")\n",
    "# Read the document and print its contents\n",
    "doc1Txt = doc1.read()\n",
    "print(doc1Txt)\n",
    "\n",
    "# Get a second document, normalize it, and remove stop words\n",
    "!curl https://raw.githubusercontent.com/MicrosoftLearning/AI-Introduction/master/files/Gettysburg.txt -o Gettysburg.txt\n",
    "doc2 = open(\"Gettysburg.txt\", \"r\")\n",
    "doc2Txt = doc2.read()\n",
    "print (doc2Txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call the Text Analytics Service to Determine Key Phrases in the Documents\n",
    "One of the methods provided by the Text Analytics service is the ability to extract a list of key phrases from text documents, which give an insight into the core topics discussed in the document.\n",
    "\n",
    "Run the following cell to call the **keyPhrases** method of the Text Analytics service and extract the key phrases for the text documents you have loaded so far in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import http.client, urllib.request, urllib.parse, urllib.error, base64, json, urllib\n",
    "\n",
    "# Define the request headers.\n",
    "headers = {\n",
    "    'Content-Type': 'application/json',\n",
    "    'Ocp-Apim-Subscription-Key': textKey,\n",
    "    'Accept': 'application/json'\n",
    "}\n",
    "\n",
    "# Define the parameters\n",
    "params = urllib.parse.urlencode({\n",
    "})\n",
    "\n",
    "# Define the request body\n",
    "body = {\n",
    "  \"documents\": [\n",
    "    {\n",
    "      \"language\": \"en\",\n",
    "      \"id\": \"1\",\n",
    "      \"text\": doc1Txt\n",
    "    },\n",
    "    {\n",
    "          \"language\": \"en\",\n",
    "          \"id\": \"2\",\n",
    "          \"text\": doc2Txt\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "try:\n",
    "    # Execute the REST API call and get the response.\n",
    "    conn = http.client.HTTPSConnection(textAnalyticsURI)\n",
    "    conn.request(\"POST\", \"/text/analytics/v2.0/keyPhrases?%s\" % params, str(body), headers)\n",
    "    response = conn.getresponse()\n",
    "    data = response.read().decode(\"UTF-8\")\n",
    "\n",
    "    # 'data' contains the JSON data. The following formats the JSON data for display.\n",
    "    parsed = json.loads(data)\n",
    "    for document in parsed['documents']:\n",
    "        print(\"Document \" + document[\"id\"] + \" key phrases:\")\n",
    "        for phrase in document['keyPhrases']:\n",
    "            print(\"  \" + phrase)\n",
    "        print(\"---------------------------\")\n",
    "    conn.close()\n",
    "\n",
    "except Exception as e:\n",
    "    print('Error:')\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these key phrases, it's reasonably clear that the first document is about freedom and nationhood, while the second is about software services for AI.\n",
    "\n",
    "### Perform Sentiment Analysis\n",
    "Another common AI requirement is to determine the sentiment associated with some text. For example, you might analyze tweets that include your organization's twitter handle to determine if they are positive or negative.\n",
    "\n",
    "Run the cell below to use the **sentiment** method of the Text Analytics service to discern the sentiment of two sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body = {\n",
    "  \"documents\": [\n",
    "    {\n",
    "      \"language\": \"en\",\n",
    "      \"id\": \"1\",\n",
    "      \"text\": \"Wow! cognitive services are fantastic.\"\n",
    "    },\n",
    "    {\n",
    "      \"language\": \"en\",\n",
    "      \"id\": \"2\",\n",
    "      \"text\": \"I hate it when computers don't understand me.\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "try:\n",
    "    conn = http.client.HTTPSConnection(textAnalyticsURI)\n",
    "    conn.request(\"POST\", \"/text/analytics/v2.0/sentiment?%s\" % params, str(body), headers)\n",
    "    response = conn.getresponse()\n",
    "    data = response.read().decode(\"UTF-8\")\n",
    "    parsed = json.loads(data)\n",
    "    for document in parsed['documents']:\n",
    "        sentiment = \"negative\"\n",
    "        if document[\"score\"] >= 0.5:\n",
    "            sentiment = \"positive\"\n",
    "        print(\"Document:\" + document[\"id\"] + \" = \" + sentiment)\n",
    "    conn.close()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"[Errno {0}] {1}\".format(e.errno, e.strerror))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Working with Speech\n",
    "So far, we've seen how analyze text; but increasingly AI systems enable humans to communicate with software services through speech recognition.\n",
    "\n",
    "1. Navigate to your Speech service to open its blade.\n",
    "2. In the blade for your Speech service, click **Keys** and then copy **Key 1** to the clipboard and paste it into the **speeckKey** variable assignment value in the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speechKey = '<your key>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install the SpeechRecognition Package\n",
    "You can call the Speech API by sending HTTP requests containing a JSON payload as you did for the Text Analytics API. However, the Speech API is supported in a pre-built Python package named **SpeechRecognition**, which makes it easier to use. You can find out more about this package at https://pypi.python.org/pypi/SpeechRecognition.\n",
    "\n",
    "Run the following cell to install the **SpeechRecognition** package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Custom Vision Service SDK  in the current Jupyter kernel\n",
    "import sys\n",
    "!{sys.executable} -m pip install SpeechRecognition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listen to the Speech\n",
    "In this exercise, you will use a .wav audio file. To hear the speech you will analyze, run the cell below (this assumes you have audio capabilities in your computer!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "\n",
    "!curl https://raw.githubusercontent.com/MicrosoftLearning/AI-Introduction/master/files/RainSpain.wav -o RainSpain.wav\n",
    "\n",
    "IPython.display.Audio('RainSpain.wav', autoplay=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call the  Speech Service to Transcribe the Audio\n",
    "Hopefully you understood what was said in the audio file.\n",
    "\n",
    "Let's see how the Speech API does!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition as sr\n",
    "\n",
    "# Read the audio file\n",
    "r = sr.Recognizer()\n",
    "with sr.AudioFile('RainSpain.wav') as source:\n",
    "    audio = r.record(source)\n",
    "    \n",
    "# Alternative code to use mic input (only works in local Jupyter - not in Azure Notebooks)\n",
    "# r = sr.Recognizer()\n",
    "# with sr.Microphone() as source:\n",
    "#     print(\"Say something!\")\n",
    "#     audio = r.listen(source)\n",
    "\n",
    "# transcribe speech using the Bing Speech API\n",
    "try:\n",
    "    transcription = r.recognize_bing(audio, key=speechKey)\n",
    "    print(\"Here's what I heard:\")\n",
    "    print('\"' + transcription + '\"')\n",
    "    \n",
    "except sr.UnknownValueError:\n",
    "    print(\"The audio was unclear\")\n",
    "except sr.RequestError as e:\n",
    "    print (e)\n",
    "    print(\"Something went wrong :-(; {0}\".format(e))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Language Understanding Intelligence Service (LUIS)\n",
    "Increasingly, we expect computers to be able to use AI in order to understand spoken or typed commands in natural language. For example, we want to be able to say \"switch on the light\" or \"put the light on\", and have an AI-powered device understand the command and take appropriate action.\n",
    "\n",
    "### Provision the Language Understanding Intelligence Service (LUIS)\n",
    "The Microsoft cognitive services include the Language Understanding Intelligence Service (LUIS), which enables you to define *intents* that are applied to *entities* based on *utterances*.\n",
    "\n",
    "\n",
    "### Create a LUIS App\n",
    "To implement natural language understanding with LUIS, you must first create an app; and then add intents, utterances, and entities to define the commands you want the app to understand.\n",
    "1. Open a new browser tab and navigate to https://www.luis.ai/.\n",
    "2. Sign in using the Microsoft account associated with your Azure subscription. If this is the first time you have signed into LUIS, you may need to grant the app some permissions to access your account details, and then fill in some information and accept the terms of use.\n",
    "3. If a message prompting you to complete a tutorial in which you will create a *Scheduler* app is displayed, close it (you can complete this tutorial later - for now, we'll focus on a simpler example).\n",
    "4. Click **New App** and create a new app with the following settings:\n",
    "  * **Name**: Simple Home Automation\n",
    "  * **Culture**: English\n",
    "  * **Description**: A basic home automation example\n",
    "5. In the pane on the left, click **Intents**. Then click **Add Intent**, and add an intent with the name **Light On**.\n",
    "6. In the **Utterances** page for the **Light On** intent, type \"*switch the light on*\" and press **Enter** to add this utterance to the list.\n",
    "7. In the list of utterances, in the *switch the light on* utterance, hold the mouse over the word \"light\" so that the list shows the value *switch the [light] on*. Then in the **Search or create** box type *Light* and create a simple entity named **Light**.\n",
    "8. In the pane on the left, click **Intents** and click **Add Intent**, to add a second intent with the name **Light Off**.\n",
    "9. In the **Utterances** page for the **Light Off** intent, type \"*switch the light off*\" and press **Enter** to add this utterance to the list.\n",
    "10. In the list of utterances, in the *switch the light on* utterance, hold the mouse over the word \"light\" so that the list shows the value *switch the [light] on*. Then click **[light]** select the *Light* entity you created previously.\n",
    "11. At the top of the page, click **Train** to train the application\n",
    "12. After the app has been trained, click **Test**, and then in the test pane, enter the following utterances and verify that they are correctly interpreted as commands for the *Light On* and *Light Off* intents respectively:\n",
    "    * *turn on the light*\n",
    "    * *put the light off*\n",
    "13. IAt the top of the page, click **Publish**. Then click **Publish to production slot**.\n",
    "14. After the app has been published, note the **Endpoint** URL that is generated for your app - this includes the location where you provisioned the service, your app ID, and the key assigned to the app.\n",
    "\n",
    "### Consume the LUIS App\n",
    "Now that you have published your LUIS app, you can consume it from a client application by making HTTP requests that include a query string. The query will be used to identify the most likely intent, which will be returned to the calling client as in JSON response.\n",
    "\n",
    "Modify the **endpointUrl** variable declaration in the cell below to reflect the endpoint URL for your app. Then run the cell, and enter a command when prompted to call your service and interpret the command. The JSON response is shown with an appropriate image for each command.\n",
    "\n",
    "Try the following commands:\n",
    "* *Switch on the light*\n",
    "* *Turn on the light*\n",
    "* *Turn off the light*\n",
    "* *Could you put the light on please?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib.pyplot import imshow\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import json \n",
    "\n",
    "# Set up API configuration\n",
    "endpointUrl = \"<your endpoint>\"\n",
    "\n",
    "# prompt for a command\n",
    "command = input('Please enter a command: \\n')\n",
    "\n",
    "# Call the LUIS service and get the JSON response\n",
    "endpoint = endpointUrl + command.replace(\" \",\"+\")\n",
    "response = requests.get(endpoint)\n",
    "data = json.loads(response.content.decode(\"UTF-8\"))\n",
    "print (data)\n",
    "\n",
    "# Identify the top scoring intent\n",
    "intent = data[\"topScoringIntent\"][\"intent\"]\n",
    "if (intent == \"Light On\"):\n",
    "    img_url = 'https://raw.githubusercontent.com/MicrosoftLearning/AI-Introduction/master/files/LightOn.jpg'\n",
    "elif (intent == \"Light Off\"):\n",
    "    img_url = 'https://raw.githubusercontent.com/MicrosoftLearning/AI-Introduction/master/files/LightOff.jpg'\n",
    "else:\n",
    "    img_url = 'https://raw.githubusercontent.com/MicrosoftLearning/AI-Introduction/master/files/Dunno.jpg'\n",
    "\n",
    "# Get the appropriate image and show it\n",
    "response = requests.get(img_url)\n",
    "img = Image.open(BytesIO(response.content))\n",
    "imshow(img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Speech Recognition and Natural Language Understanding\n",
    "An obvious next step is to combine speech recognition with natural language understanding so that a spoken command can be interpreted and the appropriate action taken.\n",
    "\n",
    "### Download Command Audio\n",
    "Let's start by downloading and playing some spoken commands for our home automation system. Run the two cells under this to hear the commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "\n",
    "# Get the \"lights on\" command\n",
    "!curl https://raw.githubusercontent.com/MicrosoftLearning/AI-Introduction/master/files/LightOn.wav -o LightOn.wav\n",
    "    \n",
    "IPython.display.Audio('LightOn.wav', autoplay=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the \"lights on\" command\n",
    "!curl https://raw.githubusercontent.com/MicrosoftLearning/AI-Introduction/master/files/LightOff.wav -o LightOff.wav\n",
    "    \n",
    "IPython.display.Audio('LightOff.wav', autoplay=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transcribe and Interpret the \"Light On\" Command\n",
    "Now you can use the Bing Speech API to transcribe the command, and then use your LUIS app to interpret the command and take the appropriate action.\n",
    "\n",
    "You should have already installed the **SpeechRecognition** package, set the **speechKey** for your Bing Speech service, and set the **apiUrl**, **appId**, and **appKey** for your LUIS app (if you have closed and re-opened the notebook, re-run the appropriate cells above to set these).\n",
    "\n",
    "Run the cell below that to test the \"Light On\" command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import speech_recognition as sr\n",
    "from matplotlib.pyplot import imshow\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import json \n",
    "\n",
    "# Specify which audio file to use\n",
    "audioFile = \"LightOn.wav\"\n",
    "\n",
    "# Read the audio file\n",
    "r = sr.Recognizer()\n",
    "with sr.AudioFile(audioFile) as source:\n",
    "    audio = r.record(source)\n",
    "\n",
    "try:\n",
    "    # transcribe speech using the Bing Speech API\n",
    "    transcription = r.recognize_bing(audio, key=speechKey)\n",
    "    \n",
    "    # Call the LUIS service and get the JSON response\n",
    "    endpoint = endpointUrl + transcription.replace(\" \",\"+\")\n",
    "    response = requests.get(endpoint)\n",
    "    data = json.loads(response.content.decode(\"UTF-8\"))\n",
    "\n",
    "    # Identify the top scoring intent\n",
    "    intent = data[\"topScoringIntent\"][\"intent\"]\n",
    "    if (intent == \"Light On\"):\n",
    "        img_url = 'https://raw.githubusercontent.com/MicrosoftLearning/AI-Introduction/master/files/LightOn.jpg'\n",
    "    elif (intent == \"Light Off\"):\n",
    "        img_url = 'https://raw.githubusercontent.com/MicrosoftLearning/AI-Introduction/master/files/LightOff.jpg'\n",
    "    else:\n",
    "        img_url = 'https://raw.githubusercontent.com/MicrosoftLearning/AI-Introduction/master/files/Dunno.jpg'\n",
    "\n",
    "    # Get the appropriate image and show it\n",
    "    response = requests.get(img_url)\n",
    "    img = Image.open(BytesIO(response.content))\n",
    "    imshow(img)\n",
    "    \n",
    "except sr.UnknownValueError:\n",
    "    print(\"Bing Speech could not understand audio\")\n",
    "except sr.RequestError as e:\n",
    "    print (e)\n",
    "    print(\"Could not request results from the Bing Speech service; {0}\".format(e))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transcribe and Interpret the \"Light Off\" Command\n",
    "Modify the cell above to set the **audioFile** variable to the **LightOff.wav** audio file, and then run the cell again to test it"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
